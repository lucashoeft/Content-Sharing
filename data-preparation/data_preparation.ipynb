{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42899d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2364c1",
   "metadata": {},
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca80f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date from which on the tweets are selected\n",
    "date_parameter_tweet_selecton = pd.to_datetime('2021-09-26') # Voting: '2021-09-26', Official Start: '2021-10-26'\n",
    "\n",
    "# Date from which on accounts should tweeted at least once, otherwise they are droped\n",
    "date_parameter_account_selection = pd.to_datetime('2021-08-27') # 30 days prior voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e407ed8",
   "metadata": {},
   "source": [
    "# Import of intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcfc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdb_list\n",
    "mdb_list = pd.read_csv('../data/intermediate/mdb_list/mdb_list.csv', sep=\";\")\n",
    "\n",
    "# mdb_twitter_list\n",
    "mdb_twitter_list = pd.read_csv('../data/intermediate/mdb_twitter_list/mdb_twitter_list.csv', sep=\";\", na_values=\"NA\")\n",
    "\n",
    "# tweet_list\n",
    "tweet_list = pd.DataFrame()\n",
    "pattern = re.compile(\"tweet_list_*\")\n",
    "directory = \"../data/intermediate/tweet_list/\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if pattern.match(filename):\n",
    "        filepath = directory + filename\n",
    "        user_tweet_list = pd.read_csv(filepath, sep=\";\", lineterminator='\\n')\n",
    "        tweet_list = pd.concat([tweet_list, user_tweet_list], axis=0)\n",
    "        \n",
    "# user_list\n",
    "user_list = pd.read_csv('../data/intermediate/user_list/user_list.csv', sep=\";\", na_values=\"\", dtype={'twitter_id': str})\n",
    "user_list_secondary = pd.read_csv('../data/intermediate/user_list/user_list_secondary.csv', sep=\";\", na_values=\"\", dtype={'twitter_id': str})\n",
    "user_list = pd.concat([user_list, user_list_secondary], axis=0, ignore_index=True)\n",
    "\n",
    "# user_friendships\n",
    "user_friendships = pd.read_csv('../data/intermediate/user_friendships/user_friendships.csv', sep=\";\", na_values=\"\")\n",
    "user_friendships_secondary = pd.read_csv('../data/intermediate/user_friendships/user_friendships_secondary.csv', sep=\";\", na_values=\"\")\n",
    "user_friendships = pd.concat([user_friendships, user_friendships_secondary], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72079543",
   "metadata": {},
   "source": [
    "# Create final _mdb_list.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185e09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb_list.to_csv('../data/processed/mdb_list.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78938b38",
   "metadata": {},
   "source": [
    "# Create final _user_list.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27f3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bh/vwy58769197gtfy7_tb12xqw0000gn/T/ipykernel_50094/3180358286.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  user_list = user_list.append({'bundestag_id': int(11004245),\n",
      "/var/folders/bh/vwy58769197gtfy7_tb12xqw0000gn/T/ipykernel_50094/3180358286.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  user_list = user_list.append({'bundestag_id': int(11003231),\n"
     ]
    }
   ],
   "source": [
    "# Clean Up of user_list (remove NA and duplicates)\n",
    "user_list = user_list[user_list['twitter_id'].notnull()] # Remove accounts with no twitter account found\n",
    "user_list = user_list.drop_duplicates(subset=['twitter_id']) # case Knut Gerschau (has a Twitter Account now \"GerschauKnut\" but is not included in analysis)\n",
    "\n",
    "# Add Alias accounts to user_list\n",
    "user_list = user_list.append({'bundestag_id': int(11004245),\n",
    "                              'nachname': 'Baerbock',\n",
    "                              'vorname': 'Annalena',\n",
    "                              'wp': int(20),\n",
    "                              'fraktion': 'Fraktion BÜNDNIS 90/DIE GRÜNEN',\n",
    "                              'twitter_handle': 'AnnalenaBaerbockAlias',\n",
    "                              'account_type': 'person'}, ignore_index=True)\n",
    "\n",
    "user_list = user_list.append({'bundestag_id': int(11003231),\n",
    "                              'nachname': 'Scholz',\n",
    "                              'vorname': 'Olaf',\n",
    "                              'wp': int(20),\n",
    "                              'fraktion': 'Fraktion der Sozialdemokratischen Partei Deutschland',\n",
    "                              'twitter_handle': 'OlafScholzAlias',\n",
    "                              'account_type': 'person'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee8369",
   "metadata": {},
   "source": [
    "### Identify accounts were not all tweets were scraped (because of high twitter usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e51464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MatthiasHauer\n",
      "Official Tweet count: 18080.0\n",
      "Tweets downloaded: 3200\n",
      "Percantage Downloaded 17.7 %\n",
      "Oldest Tweet: 2021-11-23 05:25:58+00:00\n",
      "Date Parameter: 2021-09-26 00:00:00\n",
      "\n",
      "SaraNanni\n",
      "Official Tweet count: 17017.0\n",
      "Tweets downloaded: 3200\n",
      "Percantage Downloaded 18.8 %\n",
      "Oldest Tweet: 2021-10-04 07:54:50+00:00\n",
      "Date Parameter: 2021-09-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "user_list1 = user_list.dropna(subset=['twitter_id'])\n",
    "\n",
    "def findOldestTweet(twitter_handle):\n",
    "    user_tweets = tweet_list[tweet_list['twitter_handle'] == twitter_handle]\n",
    "    return pd.to_datetime(user_tweets.iloc[-1]['tweet_created_at'])\n",
    "\n",
    "for index, contents in user_list1.iterrows():\n",
    "    tweets_downloaded = tweet_list[tweet_list['twitter_handle'] == contents['twitter_handle']].shape[0]\n",
    "    \n",
    "    if tweets_downloaded > 0:\n",
    "        dateOldestDownloadedTweet = pd.to_datetime(findOldestTweet(contents['twitter_handle'])).tz_localize(None)\n",
    "        \n",
    "        if dateOldestDownloadedTweet > date_parameter_tweet_selecton:\n",
    "            \n",
    "            if tweets_downloaded > 100:\n",
    "                percentage_downloaded = tweet_list[tweet_list['twitter_handle'] == contents['twitter_handle']].shape[0]/contents['tweet_count']\n",
    "                \n",
    "                if percentage_downloaded < 0.9:\n",
    "                    print()\n",
    "                    print(contents['twitter_handle'])\n",
    "\n",
    "                    print('Official Tweet count:', contents['tweet_count'])\n",
    "                    print('Tweets downloaded:', tweets_downloaded)\n",
    "                    print('Percantage Downloaded', round(percentage_downloaded*100,2), '%')\n",
    "\n",
    "                    print('Oldest Tweet:', findOldestTweet(contents['twitter_handle']))\n",
    "                    print('Date Parameter:', date_parameter_tweet_selecton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3ff13",
   "metadata": {},
   "source": [
    "### Find inactive accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34917a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnetteKramme \tLatest Tweet: 2020-06-21 07:45:09+00:00\n",
      "ToniHofreiter \tLatest Tweet: 2013-10-29 11:06:47+00:00\n",
      "CarstenMuellers \tLatest Tweet: 2009-07-08 15:50:13+00:00\n",
      "GerdesMdB \tLatest Tweet: Never tweeted\n",
      "VolkmarKlein \tLatest Tweet: 2021-06-11 15:33:50+00:00\n",
      "MdBMonstadt \tLatest Tweet: 2019-10-04 13:08:55+00:00\n",
      "FechnerJohannes \tLatest Tweet: Never tweeted\n",
      "UweFe \tLatest Tweet: 2009-06-15 15:27:15+00:00\n",
      "KoobMar \tLatest Tweet: Never tweeted\n",
      "JanMetzler \tLatest Tweet: 2019-09-10 11:37:28+00:00\n",
      "MittagSusanne \tLatest Tweet: Never tweeted\n",
      "WilfriedOellers \tLatest Tweet: 2019-05-09 17:25:05+00:00\n",
      "ruetzelbernd \tLatest Tweet: 2020-09-15 15:30:16+00:00\n",
      "UdoSchiefner \tLatest Tweet: 2020-06-18 08:33:38+00:00\n",
      "NAltenkamp \tLatest Tweet: Never tweeted\n",
      "Philipp_Amthor \tLatest Tweet: Never tweeted\n",
      "AlexGauland \tLatest Tweet: 2018-07-10 13:19:11+00:00\n",
      "Jochen_Haug \tLatest Tweet: 2021-05-19 12:58:25+00:00\n",
      "Huber_AfD \tLatest Tweet: 2017-10-12 00:37:06+00:00\n",
      "JoernKoenigAfD \tLatest Tweet: 2019-06-25 17:48:25+00:00\n",
      "NBaradari \tLatest Tweet: Never tweeted\n",
      "ali_aldailami \tLatest Tweet: 2020-04-28 15:28:34+00:00\n",
      "Holger_SciTec \tLatest Tweet: Never tweeted\n",
      "EnglhardtKopf \tLatest Tweet: 2020-12-18 23:15:39+00:00\n",
      "HumpferMarkus \tLatest Tweet: 2016-03-22 16:20:26+00:00\n",
      "timkluessendorf \tLatest Tweet: Never tweeted\n",
      "MartinKroeber \tLatest Tweet: Never tweeted\n",
      "SLahrkamp \tLatest Tweet: 2021-02-15 13:20:50+00:00\n",
      "TMachalet \tLatest Tweet: 2019-03-31 09:40:17+00:00\n",
      "MMoosdorf \tLatest Tweet: Never tweeted\n",
      "natalie_pawlik \tLatest Tweet: 2021-05-17 19:14:05+00:00\n",
      "PhilippiSpd \tLatest Tweet: 2021-04-23 12:01:39+00:00\n",
      "SchaetzlJo \tLatest Tweet: Never tweeted\n",
      "anja_troff \tLatest Tweet: Never tweeted\n",
      "weisse_Lena \tLatest Tweet: 2012-10-06 10:49:38+00:00\n",
      "\n",
      "n of inactive accounts 35\n"
     ]
    }
   ],
   "source": [
    "def findLatestTweet(twitter_handle):\n",
    "    user_tweets = tweet_list[tweet_list['twitter_handle'] == twitter_handle]\n",
    "    return pd.to_datetime(user_tweets.iloc[0]['tweet_created_at'])\n",
    "\n",
    "inactiveUsers = []\n",
    "\n",
    "i = 0\n",
    "for index, contents in user_list1.iterrows():\n",
    "    tweets_downloaded = tweet_list[tweet_list['twitter_handle'] == contents['twitter_handle']].shape[0]\n",
    "    \n",
    "    if tweets_downloaded > 0:\n",
    "        dateLatestTweet = pd.to_datetime(findLatestTweet(contents['twitter_handle'])).tz_localize(None)\n",
    "        if dateLatestTweet < date_parameter_account_selection:\n",
    "            inactiveUsers.append(contents['twitter_handle'])\n",
    "            print(contents['twitter_handle'], '\\tLatest Tweet:', findLatestTweet(contents['twitter_handle']))\n",
    "            i += 1\n",
    "    else:\n",
    "        inactiveUsers.append(contents['twitter_handle'])\n",
    "        print(contents['twitter_handle'], '\\tLatest Tweet: Never tweeted')\n",
    "        i += 1\n",
    "        \n",
    "print(\"\\nn of inactive accounts\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e003684",
   "metadata": {},
   "source": [
    "### Build dataframe with all relevant users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0dfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# MDBs that deleted Twitter account\n",
    "mdb_deleted_twitter = user_list[user_list['twitter_id'].isnull()]\n",
    "\n",
    "# Remove accounts that were deleted\n",
    "final_user_list = pd.concat([user_list, mdb_deleted_twitter]).drop_duplicates(keep=False)\n",
    "\"\"\"\n",
    "\n",
    "# Remove accounts that are inactive\n",
    "user_list = user_list[~user_list.twitter_handle.isin(inactiveUsers)]\n",
    "\n",
    "# Remove duplicate (Knut Gerschau has the wrong twitter_handle / the one of Knut Abraham)\n",
    "user_list = user_list.drop_duplicates(subset='twitter_handle', keep='first')\n",
    "\n",
    "user_list.to_csv('../data/processed/user_list.csv', index=False, decimal=',', sep=\";\", float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd105b8",
   "metadata": {},
   "source": [
    "# Create final _user_friendships.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fd240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Up of user_friendships\n",
    "user_friendships = user_friendships[user_friendships['following'] == True] # Show only Edges\n",
    "user_friendships = user_friendships.drop_duplicates(subset=['source_screen_name', 'source_id', 'target_screen_name', 'target_id', 'following']) # Remove duplicates\n",
    "\n",
    "# Create Alias for Baerbock and Scholz\n",
    "user_friendships_secondary = user_friendships.replace(to_replace='ABaerbock', value='AnnalenaBaerbockAlias')\n",
    "user_friendships_secondary = user_friendships_secondary.replace(to_replace='ABaerbockArchiv', value='AnnalenaBaerbockAlias')\n",
    "user_friendships_secondary = user_friendships_secondary.replace(to_replace='OlafScholz', value='OlafScholzAlias')\n",
    "user_friendships_secondary = user_friendships_secondary.replace(to_replace='Bundeskanzler', value='OlafScholzAlias')\n",
    "user_friendships_secondary = user_friendships_secondary.drop_duplicates(subset=['source_screen_name', 'target_screen_name', 'following']) # Remove duplicates\n",
    "\n",
    "user_friendships = pd.concat([user_friendships, user_friendships_secondary], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Filter out users in user_friendships that do not appear in user_list\n",
    "user_friendships = user_friendships[user_friendships['source_screen_name'].isin(user_list['twitter_handle'])]\n",
    "user_friendships = user_friendships[user_friendships['target_screen_name'].isin(user_list['twitter_handle'])]\n",
    "\n",
    "# Remove self-loops\n",
    "user_friendships = user_friendships[user_friendships['source_screen_name'] != user_friendships['target_screen_name']]\n",
    "\n",
    "user_friendships.to_csv('../data/processed/user_friendships.csv', index=False, decimal=',', sep=\";\", float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8922539",
   "metadata": {},
   "source": [
    "# Create final _tweet_list.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a642bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create tweets for alias accounts (keep original tweets)\n",
    "tweet_list_secondary = tweet_list\n",
    "# display(tweet_list_secondary)\n",
    "\n",
    "# Create Alias for Baerbock and Scholz\n",
    "tweet_list_secondary = tweet_list_secondary.replace(to_replace='ABaerbock', value='AnnalenaBaerbockAlias')\n",
    "tweet_list_secondary = tweet_list_secondary.replace(to_replace='ABaerbockArchiv', value='AnnalenaBaerbockAlias')\n",
    "tweet_list_secondary = tweet_list_secondary.replace(to_replace='OlafScholz', value='OlafScholzAlias')\n",
    "tweet_list_secondary = tweet_list_secondary.replace(to_replace='Bundeskanzler', value='OlafScholzAlias')\n",
    "\n",
    "tweet_list = pd.concat([tweet_list, tweet_list_secondary], ignore_index=True).drop_duplicates()\n",
    "\n",
    "print(tweet_list.shape)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Filter out tweets of users that do not appear in user_list\n",
    "tweet_list = tweet_list[tweet_list['twitter_handle'].isin(user_list['twitter_handle'])]\n",
    "\n",
    "tweet_list.to_csv('../data/processed/tweet_list.csv', index=False, decimal=',', sep=\";\", float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c52fbc",
   "metadata": {},
   "source": [
    "# Create final retweet_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe93f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all retweets in the tweet_list dataframe\n",
    "retweet_tweet_list = tweet_list[tweet_list['tweet_referenced_tweet_type'] == \"retweeted\"]\n",
    "\n",
    "# Create a dataframe with all retweets that are related to tweets in the tweet_list dataframe\n",
    "relevant_retweet_tweet_list = pd.merge(retweet_tweet_list, tweet_list, left_on='tweet_referenced_tweet_id', right_on='tweet_id')\n",
    "relevant_retweet_tweet_list.drop(['twitter_id_x',\\\n",
    "    'tweet_conversation_id_x',\\\n",
    "    'tweet_in_reply_to_user_id_x',\\\n",
    "    'tweet_lang_x',\\\n",
    "    'tweet_possibly_sensitive_x',\\\n",
    "    'tweet_retweet_count_x',\\\n",
    "    'tweet_reply_count_x',\\\n",
    "    'tweet_like_count_x',\\\n",
    "    'tweet_quote_count_x',\\\n",
    "    'tweet_reply_settings_x',\\\n",
    "    'tweet_referenced_tweet_id_x',\\\n",
    "    'tweet_referenced_tweet_id_x',\\\n",
    "    'tweet_source_x',\\\n",
    "    'api_call_x',\\\n",
    "    'twitter_id_y',\\\n",
    "    'tweet_conversation_id_y',\\\n",
    "    'tweet_in_reply_to_user_id_y',\\\n",
    "    'tweet_lang_y',\\\n",
    "    'tweet_possibly_sensitive_y',\\\n",
    "    'tweet_retweet_count_y',\\\n",
    "    'tweet_reply_count_y',\\\n",
    "    'tweet_like_count_y',\\\n",
    "    'tweet_quote_count_y',\\\n",
    "    'tweet_reply_settings_y',\\\n",
    "    'tweet_source_y',\\\n",
    "    'tweet_referenced_tweet_id_y',\\\n",
    "    'tweet_referenced_tweet_type_y',\\\n",
    "    'api_call_y'], axis=1, inplace=True)\n",
    "relevant_retweet_tweet_list.rename(columns={\"tweet_author_id_x\": \"retweeter_twitter_id \",\\\n",
    "    \"tweet_created_at_x\": \"retweet_created_at\",\\\n",
    "    \"twitter_handle_x\": \"retweeter_twitter_handle\",\\\n",
    "    \"tweet_id_x\": \"retweet_tweet_id\",\\\n",
    "    \"tweet_text_x\": \"retweet_text\",\\\n",
    "    \"twitter_handle_y\": \"author_twitter_handle\",\\\n",
    "    \"tweet_id_y\": \"tweet_id\",\\\n",
    "    \"tweet_text_y\": \"tweet_text\",\\\n",
    "    \"tweet_author_id_y\": \"author_twitter_id\",\\\n",
    "    \"tweet_created_at_y\": \"tweet_created_at\"}, inplace=True)\n",
    "\n",
    "# Store results\n",
    "relevant_retweet_tweet_list.to_csv('../data/processed/retweet_list.csv', index=False, decimal=',', sep=\";\", float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e9c9e",
   "metadata": {},
   "source": [
    "# Create final quote_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2909399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all quotes in the tweet_list dataframe\n",
    "quote_tweet_list = tweet_list[tweet_list['tweet_referenced_tweet_type'] == \"quoted\"]\n",
    "\n",
    "# Create a dataframe with all quotes that are related to tweets in the tweet_list dataframe\n",
    "relevant_quote_tweet_list = pd.merge(quote_tweet_list, tweet_list, left_on='tweet_referenced_tweet_id', right_on='tweet_id')\n",
    "relevant_quote_tweet_list.drop(['twitter_id_x',\\\n",
    "    'tweet_conversation_id_x',\\\n",
    "    'tweet_in_reply_to_user_id_x',\\\n",
    "    'tweet_lang_x',\\\n",
    "    'tweet_possibly_sensitive_x',\\\n",
    "    'tweet_retweet_count_x',\\\n",
    "    'tweet_reply_count_x',\\\n",
    "    'tweet_like_count_x',\\\n",
    "    'tweet_quote_count_x',\\\n",
    "    'tweet_reply_settings_x',\\\n",
    "    'tweet_referenced_tweet_id_x',\\\n",
    "    'tweet_referenced_tweet_id_x',\\\n",
    "    'tweet_source_x',\\\n",
    "    'api_call_x',\\\n",
    "    'twitter_id_y',\\\n",
    "    'tweet_conversation_id_y',\\\n",
    "    'tweet_in_reply_to_user_id_y',\\\n",
    "    'tweet_lang_y',\\\n",
    "    'tweet_possibly_sensitive_y',\\\n",
    "    'tweet_retweet_count_y',\\\n",
    "    'tweet_reply_count_y',\\\n",
    "    'tweet_like_count_y',\\\n",
    "    'tweet_quote_count_y',\\\n",
    "    'tweet_reply_settings_y',\\\n",
    "    'tweet_source_y',\\\n",
    "    'tweet_referenced_tweet_id_y',\\\n",
    "    'tweet_referenced_tweet_type_y',\\\n",
    "    'api_call_y'], axis=1, inplace=True)\n",
    "relevant_quote_tweet_list.rename(columns={\"tweet_author_id_x\": \"quoter_twitter_id \",\\\n",
    "    \"tweet_created_at_x\": \"quote_created_at\",\\\n",
    "    \"twitter_handle_x\": \"quoter_twitter_handle\",\\\n",
    "    \"tweet_id_x\": \"quote_tweet_id\",\\\n",
    "    \"tweet_text_x\": \"quote_text\",\\\n",
    "    \"twitter_handle_y\": \"author_twitter_handle\",\\\n",
    "    \"tweet_id_y\": \"tweet_id\",\\\n",
    "    \"tweet_text_y\": \"tweet_text\",\\\n",
    "    \"tweet_author_id_y\": \"author_twitter_id\",\\\n",
    "    \"tweet_created_at_y\": \"tweet_created_at\"}, inplace=True)\n",
    "\n",
    "# Store results\n",
    "relevant_quote_tweet_list.to_csv('../data/processed/quote_list.csv', index=False, decimal=',', sep=\";\", float_format='%.0f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
